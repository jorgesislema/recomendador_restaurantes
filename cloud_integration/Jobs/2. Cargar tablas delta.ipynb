{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7147af8-e56b-4457-9b7a-30f2f3f2a06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Cargamos los datos prosesados en la tabla delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf6cb5e-db6f-4327-9bd8-f559ab4ed013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados desde: dbfs:/mnt/ingest_data/processed/etl_datos_de_prueba.parquet\nTodos los datos se han cargado exitosamente en la tabla Delta.\n"
     ]
    }
   ],
   "source": [
    "# Cargar esquema de la tabla Delta existente\n",
    "tabla_delta = \"aprendisaje.default.servicio_restaurantes\"\n",
    "ruta_processed = \"dbfs:/mnt/ingest_data/processed/\"\n",
    "\n",
    "try:\n",
    "    esquema_delta = spark.table(tabla_delta).schema\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el esquema de la tabla Delta: {e}\")\n",
    "    raise\n",
    "\n",
    "# Listar archivos procesados\n",
    "archivos_procesados = [f.path for f in dbutils.fs.ls(ruta_processed)]\n",
    "\n",
    "for archivo in archivos_procesados:\n",
    "    # Leer archivo procesado\n",
    "    df_spark = spark.read.format(\"parquet\").load(archivo)\n",
    "    \n",
    "    # Validar columnas\n",
    "    columnas_comunes = [col for col in esquema_delta.fieldNames() if col in df_spark.columns]\n",
    "    \n",
    "    # Validar tipos de datos y convertir si es necesario\n",
    "    df_alineado = df_spark.select(\n",
    "        *[\n",
    "            df_spark[col].cast(esquema_delta[col].dataType).alias(col)\n",
    "            for col in columnas_comunes\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Escribir datos en la tabla Delta\n",
    "    try:\n",
    "        df_alineado.write.format(\"delta\").mode(\"append\").saveAsTable(tabla_delta)\n",
    "        print(f\"Datos cargados desde: {archivo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos desde {archivo}: {e}\")\n",
    "\n",
    "print(\"Todos los datos se han cargado exitosamente en la tabla Delta.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2Cargar tablas delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
